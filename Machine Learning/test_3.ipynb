{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af251af-2103-409b-a735-98998a3708d8",
   "metadata": {},
   "source": [
    "# Test-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96882c-6fbc-40e7-9c46-02ec42096426",
   "metadata": {},
   "source": [
    "## Algorithm Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94332262-6ef5-4897-8d50-29e86c933574",
   "metadata": {},
   "source": [
    "### 1. How does regularization (L1 and L2) help in preventing overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bc276-6545-4bbe-ab20-6c81aebe6889",
   "metadata": {},
   "source": [
    "Regularization reduces model complexity to prevent overfitting.\n",
    "\n",
    "- L1 : Adds a penalty that can make some feature weights zero, helping with feature selection.\n",
    "- L2 : Adds a penalty to shrink feature weights, making the model simpler and less sensitive to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4cc0b-21bb-4554-8b13-b1defde0d671",
   "metadata": {},
   "source": [
    "### 2. Why is feature scaling important in gradient descent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76976bb8-00da-4f95-a237-045c0533a530",
   "metadata": {},
   "source": [
    "Feature scaling ensures that all features are on the same scale. Without it, features with larger values can dominate the learning process, making the algorithm slower or harder to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c1958-0b5e-46c3-9bf9-32fea75de404",
   "metadata": {},
   "source": [
    "## Problem Solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893ad5a-8548-4a37-ad4f-c65a4e37f8aa",
   "metadata": {},
   "source": [
    "### 1. How would you handle missing values in a dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bae683-c673-4270-9912-33de19754917",
   "metadata": {},
   "source": [
    "You can handle missing data in a few ways:\n",
    "\n",
    "Remove rows or columns with missing data.\n",
    "- Impute missing values using the mean/median (for numerical data) or mode (for categorical data).\n",
    "- Use models like Random Forest or XGBoost, which handle missing data naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af894247-a350-4f37-bb5c-c316cd867777",
   "metadata": {},
   "source": [
    "### 2. Design a pipeline for building a classification model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60864b50-7935-4576-8292-1e6bd5dd049d",
   "metadata": {},
   "source": [
    "- Collect data (e.g., load CSV).\n",
    "- Preprocess data: Handle missing values, encode categorical variables, scale features.\n",
    "- Split data into training and test sets (e.g., 80/20).\n",
    "- Choose a model (e.g., Logistic Regression, Decision Tree).\n",
    "- Train the model on the training set.\n",
    "- Evaluate performance (e.g., accuracy).\n",
    "- Tune hyperparameters to improve the model.\n",
    "- Deploy the model for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e49b46-dd36-4163-af9c-bf283769dc20",
   "metadata": {},
   "source": [
    "### Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cb5c2-cbf0-405c-b0a1-176c267c2c14",
   "metadata": {},
   "source": [
    "### 1. Implement a Decision Tree classifier using Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3005-47cc-42b4-9cb5-2c241e7a021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27434e27-d014-40ae-8abb-30c6856c968e",
   "metadata": {},
   "source": [
    "### 2. Split the data into training and test sets (80-20 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149ed04-f4bc-4071-b4fb-1e4b4eac6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862cbf2-22a8-4f25-87df-9a97fe98dd27",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b9c97-a606-4088-b17b-a112ab8243f4",
   "metadata": {},
   "source": [
    "### 1. A company wants to predict employee attrition. What kind of problem is this? Which algorithms would you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a989e-ed14-45a4-9ce4-413942b5e225",
   "metadata": {},
   "source": [
    "This is a binary classification problem.\n",
    "\n",
    "- Logistic Regression: Simple and interpretable.\n",
    "- Decision Trees: Can capture non-linear patterns.\n",
    "- Random Forest: Combines multiple trees for better accuracy.\n",
    "- Gradient Boosting (XGBoost): Very accurate, especially for structured data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
